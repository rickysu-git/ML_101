{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS Dataset Walkthrough\n",
    "\n",
    "In this Notebook, we'll work through some basic Machine Learning Algorithms, and apply them on the IRIS dataset. For anyone who is unfamiliar with the IRIS dataset, you can read about it [here](https://archive.ics.uci.edu/ml/datasets/iris). It's basically the \"ML 101\" dataset.\n",
    "\n",
    "## Algorithms to Walkthrough\n",
    "1. Logistic Regression\n",
    "2. Support Vector Machine\n",
    "3. Decision Tree\n",
    "4. Random Forest (Ensemble/Bagging)\n",
    "5. Bonus - Mini Neural Network\n",
    "\n",
    "*As you can probably already tell, this will be a supervised task.*\n",
    "*We'll be using scikit-learn for the first 4, and Keras for the last.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll start by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the go-to dataframe library for Python\n",
    "import pandas as pd \n",
    "\n",
    "# Read csv\n",
    "iris_df = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we'll explore it a bit\n",
    "\n",
    "It's important to explore your data to get a feel of what each feature's data type is, it's range, mean, median, mode, etc., all that good stuff. A very quick and simple way of doing this is to use `describe()`, which gives you the 5 number summary, plus a little more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n",
      "\n",
      "\n",
      "\n",
      "Number of datapoints:  150\n",
      "\n",
      "\n",
      "\n",
      "               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
      "count  150.000000     150.000000    150.000000     150.000000    150.000000\n",
      "mean    75.500000       5.843333      3.054000       3.758667      1.198667\n",
      "std     43.445368       0.828066      0.433594       1.764420      0.763161\n",
      "min      1.000000       4.300000      2.000000       1.000000      0.100000\n",
      "25%     38.250000       5.100000      2.800000       1.600000      0.300000\n",
      "50%     75.500000       5.800000      3.000000       4.350000      1.300000\n",
      "75%    112.750000       6.400000      3.300000       5.100000      1.800000\n",
      "max    150.000000       7.900000      4.400000       6.900000      2.500000\n"
     ]
    }
   ],
   "source": [
    "print(iris_df.head())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Number of datapoints: \", len(iris_df))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(iris_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice how it didn't describe the `Species` column. We can do that on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Iris-setosa': 50, 'Iris-versicolor': 50, 'Iris-virginica': 50}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(iris_df['Species'], return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's three different species, each with 50 datapoints.\n",
    "\n",
    "## Balancing your dataset\n",
    "\n",
    "It's important to have a well balanced dataset. Here, we have 50 datapoints/instances of each species in the dataset. What would happen if your dataset wasn't balanced? If you were to give a machine 10000 instances of `Iris-setosa`, and 1 instance for each of `Iris-versicolor` and `Iris-virginica`, what do you think would happen? The machine would only learn the features of `Iris-setosa`, and nothing about the other two species. It would then be in the machine's favor to essentially *always* predict that a new flower is an `Iris-setosa`, which is not what we want. This was an extreme example, but you get the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training and testing\n",
    "\n",
    "Here, we'll split the dataset into training and testing sets. As the names suggest, we will train our algorithm on the training set, and then test it on the testing set. On larger datasets, we may also split out a `validation` set. The validation set is used for intermittent testing. So we would train on the training set, and \"validate\" the algorithm on the validation set. We still WOULD NOT touch the testing set though. A common split ratio would be 70/20/10 or 70/15/15 for the training, validation, and testing set sizes, respectively. But for our purpose, we'll just do a 80/20 split for training and testing.\n",
    "\n",
    "### For our walkthrough here, we'll be using the four features `SepalLengthCm`, `SepalWidthCm`, `PetalLengthCm`, and `PetalWidthCm` to predict `Species`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll shuffle the dataset first\n",
    "iris_df = iris_df.sample(frac=1)\n",
    "\n",
    "# Remove the Id column\n",
    "del iris_df['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out the features and label\n",
    "X = iris_df.drop('Species', axis=1)\n",
    "y = iris_df['Species']\n",
    "\n",
    "# Let's turn these into numpy arrays for easier manipulation\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.4, 3.2, 1.3, 0.2],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [5.1, 3.7, 1.5, 0.4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spit into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=43)\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "\n",
    "If you know `Linear Regression`, this should look pretty familiar. As a reminder, `Linear Regression` tries to map a line (it's really a hyperplane) that best fits a set of data points, using the `least squares` method. There is no limit to the domain/range that it predicts on (can be (-inf,inf)). For `Logistic Regression`, we have to limit our predictions to the domain (0,1), to predict probabilities, essentially a binary yes/no situation. In a nutshell (there's much more to this), `Logistic Regression` takes the idea of `Linear Regression`, and maps it to the log scale, and using the [Sigmoid function](https://en.wikipedia.org/wiki/Logistic_function) to guarantee that the output is in the (0,1) range. I use the (0,1) range here to explain in the binary case, but what about the N case, such as the IRIS dataset, where there are 3 classes? Essentially, the [loss function](https://en.wikipedia.org/wiki/Loss_function) is switched to use a [softmax function](https://en.wikipedia.org/wiki/Softmax_function) to extend the algorithm to multi-class. The softmax function is also used heavily by Neural Networks.\n",
    "\n",
    "[Doc page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xte364/Library/Python/3.7/lib/python/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_classifier = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just trained a classifier. Yes, it's literally one line.\n",
    "There's many more parameters you can set. Check the scikit-learn doc page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and metrics\n",
    "\n",
    "Let's first see how well we did on just the training set. That is, we'll check our accuracy on the training set. The logic/intuition here is that if we train an algorithm on the training data, it better predict really well on the training data. Otherwise, we have a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the training data\n",
    "LR_training_predictions = LR_classifier.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train, LR_training_predictions)\n",
    "\n",
    "# Hopefully you get something close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now here's the real test. We'll predict on the testing data now, which the algorithm hasn't seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_testing_predictions = LR_classifier.predict(X_test)\n",
    "accuracy_score(y_test, LR_testing_predictions)\n",
    "\n",
    "# You probably get something slightly lower than your training accuracy, which is correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helpful way to visualize what we got right/wrong is with a `confusion matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x120dcee48>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEKCAYAAAAPVd6lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5xXVb3/8dd7LoAIoggaIAoWal4SkTKvjZcSq6NZpqlpeSqztLKsThcf5s9THj1lnswrcDxYeSmtFC+J5eVg5gVURMAbB2+IpIigoOIw8/n9sdfIl3Fmvt+BmfnuPbyfj8d+sPfaa6+9vtuvn++atddeWxGBmZkVV021K2BmZuvHgdzMrOAcyM3MCs6B3Mys4BzIzcwKzoHczKzgHMjNzHqQpMslvSRpTjv7JekCSfMlzZY0rlyZDuRmZj1rCjChg/2HAGPSciJwSbkCHcjNzHpQREwHlnaQ5TDgN5G5D9hU0rCOyqzrygpa5wwZXBujRtZXuxq59eTs/tWugvUCr/PqkogYuj5lHLz/xvHK0qaK8j44e9Vc4K2SpIkRMbETpxsBPF+yvTClvdjeAQ7kVTRqZD0PTBtZ7Wrk1sHDx1a7CtYL/C2ue3Z9y3hlaRMPTNu6ory1w556KyLGr+85O8OB3MysjACaae6p070AlLbwtkpp7XIfuZlZGUHQGE0VLV1gKnB8Gr3yYWB5RLTbrQJukZuZVaSrWuSSrgYagCGSFgI/AeoBIuJS4Bbg48B84A3ghHJlOpCbmZURBE1dNOV3RBxdZn8AJ3emTAdyM7MKNJPfdzc4kJuZlRFAkwO5mVmxuUVuZlZgATTm+LWYDuRmZmUE4a4VM7NCC2jKbxx3IDczKyd7sjO/HMjNzMoSTajalWiXA7mZWRnZzU4HcjOzwsrGkTuQm5kVWrNb5GZmxeUWuZlZwQWiKcezfjuQm5lVwF0rZmYFFoi3o7ba1WiXA7mZWRnZA0HuWjEzKzTf7DQzK7AI0RRukZuZFVqzW+RmZsWV3ezMb7jMb83MzHLCNzvNzHqBJo8jNzMrLj/ZaWbWCzR71IqZWXFlk2Y5kJuZFVYgGnP8iH5+f2KsR5337ZEcuctOnLj/9tWuSm6Nb3iNyXc/zv/c8xhHnvLPalcnd3rz9YmApqipaKmG3AZySSs62PePbjzvj7qr7Dz72FFL+dmVC6pdjdyqqQlOPvsFTj92NF9p2J79D1vG1mPeqna1cqP3Xx/RXOFSDbkN5G2RVAcQEXt142k2yEC+y4dXMnCzpmpXI7e23+0NFj3Th8XP9WV1Yw133bApex68vNrVyo3efn0Ct8jXi6QGSXdLmgrMS2kr0r/DJE2XNEvSHEn7tnH8TpIeSHlmSxqT0j9fkn6ZpFpJ5wAbpbQrU77vpLLnSDo1pW0s6WZJj6T0o1L6GZJmpLSJkvI78NQ6ZfP3NPLyoj7vbC95sZ4hwxqrWKN82RCuTxM1FS3VUJSbneOAnSPi6VbpxwDTIuJnkmqB/m0cexLwq4i4UlIfoFbS+4GjgL0jolHSxcCxEfEDSadExFgASbsDJwB7AALul/S/wLbAooj4RMo3KJ3rwog4K6X9FvgkcGOXXQUzq4pAfrFEF3igjSAOMAO4XFI9cH1EzGojz73AjyVtBfwpIp6SdCCwOzAjNZo3Al5q49h9gD9HxEoASX8C9gVuBc6TdC5wU0TcnfLvL+n7ZD8og4G5tArkkk4ETgTYekRRLr+9srieocPffmd7yLBGlrxYX8Ua5Utvvz4BNOZ4rpXcd60kK9tKjIjpwH7AC8AUScdLOjx1jcySND4irgIOBd4EbpF0AFnr+oqIGJuW7SPizEorExFPkv2V8Cjw09Sl0g+4GDgiInYBJgH92jh2YkSMj4jxQzfP73AmW9sTs/ozYvTbbDlyFXX1zTQctoz7bhtU/sANRO+/PqKpwqUa8vsTUwFJ2wALI2KSpL7AuIg4FfhzSZ5tgQURcYGkrYEPALcBN0g6PyJekjQYGBgRzwKNkuojohG4m+wH4hyy4H84cJyk4cDSiPidpGXAl1kTtJdIGgAcAVzXE9ehK/zH17Zh9r0DWL60jmN335HjTlvMhGOWVrtaudHcJC768QjOvmoBNbVw2zWDefbJd/1Ob7B6+/UJ/GRnd2oAviepEVgBHN9GniPJgm8jsBg4OyKWSjoduE1SDdAInAw8C0wEZkt6KCKOlTQFeCCVNTkiHpZ0MPBzSc3p2K9FxDJJk4A56Twzuukzd4sfXvJstauQezPu2IQZd2xS7WrkVm+/Pnl+Q5Aiotp12GCN37VfPDBtZLWrkVsHDx9b7SpYL/C3uO7BiBi/PmWM2GnT+Pof9qko7+k739zh+SRNAH4F1JI1Ds9ptX9r4Apg05TnBxFxS0fnLHqL3Mys22U3O9f/nlYaXXcR8FFgIdmAi6kRMa8k2+nAHyLiEkk7ArcAozoq14HczKysLntn54eA+RGxAEDSNcBhpGdkkgBa+qgGAYvKFepAbmZWRnazs+I+8iGSZpZsT4yIiWl9BPB8yb6FZM+plDqT7P7dN4CNgYPKndCB3MysAp14anPJevbJHw1MiYjzJO0J/FbSzhHR3N4BDuRmZmV04ZOdLwClIxy2SmmlvgRMAIiIe9MzKkNo+6FFoDgPBJmZVVUzNRUtZcwAxkganaYM+RwwtVWe54ADAdJ0Iv2Alzsq1C1yM7MyIqCxef3bvRGxWtIpwDSyoYWXR8RcSWcBMyNiKnAaMEnSt8m6578YZcaJO5CbmZWRda10TQdGGhN+S6u0M0rW5wF7d6ZMB3Izswrk+clOB3IzszI6OfywxzmQm5mV1XVdK93BgdzMrALVeh9nJRzIzczKyEat5Pf9AQ7kZmZl+FVvZma9gLtWzMwKzKNWzMx6AY9aMTMrsAix2oHczKzY3LViZlZg7iM3M+sFHMjNzArM48jNzHoBjyM3MyuwCFjdBS+W6C4O5GZmFXDXiplZgbmP3MysFwgHcjOzYvPNTjOzAotwH7mZWcGJJo9aMTMrNveRW5uenN2fg4ePrXY1cmvaolnVrkLu+fvTMzzXiplZ0UXWT55XDuRmZhXwqBUzswIL3+w0Mys+d62YmRWcR62YmRVYhAO5mVnhefihmVnBuY/czKzAAtHsUStmZsWW4wY5+f2JMTPLi3Szs5KlHEkTJD0hab6kH7ST50hJ8yTNlXRVuTLdIjczq0QXNMkl1QIXAR8FFgIzJE2NiHklecYAPwT2johXJW1Rrly3yM3MKtBFLfIPAfMjYkFEvA1cAxzWKs9XgIsi4tXsvPFSuULbbZFL+jUd/AZFxDfLFW5m1hsE0Nxc8fDDIZJmlmxPjIiJaX0E8HzJvoXAHq2O3w5A0j1ALXBmRNza0Qk76lqZ2cE+M7MNRwCVjyNfEhHj1+NsdcAYoAHYCpguaZeIWNbRAW2KiCtKtyX1j4g31qNyZmaF1UXjyF8ARpZsb5XSSi0E7o+IRuBpSU+SBfYZ7RVato9c0p6S5gGPp+1dJV3cycqbmRVbVLh0bAYwRtJoSX2AzwFTW+W5nqw1jqQhZF0tCzoqtJKbnf8FHAy8AhARjwD7VXCcmVkvUdmNznI3OyNiNXAKMA14DPhDRMyVdJakQ1O2acArqQF9J/C9iHilo3IrGn4YEc9La1WwqZLjzMx6jS56IigibgFuaZV2Rsl6AN9JS0UqCeTPS9oLCEn1wLfIfknMzDYMAVH5qJUeV0nXyknAyWTDZhYBY9O2mdkGRBUuPa9sizwilgDH9kBdzMzyK8eTrVQyamVbSTdKelnSS5JukLRtT1TOzCw3umbUSreopGvlKuAPwDBgOHAtcHV3VsrMLFdaHgiqZKmCSgJ5/4j4bUSsTsvvgH7dXTEzszzJXvdWfqmGjuZaGZxW/5KmWryG7HfpKFoNnTEz6/VyPGqlo5udD5IF7pbaf7VkX5BNs2hmtkFQjm92djTXyuierIiZWW5V8UZmJSp6slPSzsCOlPSNR8RvuqtSZmb5Ur0bmZUoG8gl/YRsApcdyfrGDwH+DjiQm9mGI8ct8kpGrRwBHAgsjogTgF2BQd1aKzOzvGmucKmCSgL5mxHRDKyWtAnwEmvPp2u9wPiG15h89+P8zz2PceQp/6x2dXLnvG+P5MhdduLE/bevdlVyq1d/h3rBOPKZkjYFJpGNZHkIuLfcQZJWdLDvHxXXsBtIGi7punU89i5J6/P2j9ypqQlOPvsFTj92NF9p2J79D1vG1mPeqna1cuVjRy3lZ1d2OCX0Bm1D+A4pKluqoWwgj4ivR8SyiLiU7M3PX0hdLJ0mqS6Vude6HL+u52stIhZFxBE9VIfanjjP+th+tzdY9EwfFj/Xl9WNNdx1w6bsefDyalcrV3b58EoGbubZm9uzQXyHiviIvqRxrRdgMFCX1isiqUHS3ZKmAvNS2or07zBJ0yXNkjRH0r6tjh0k6VlJNWl7Y0nPS6qX9F5Jt0p6MJW/Q8ozRdKlku4H/lPSR1L5syQ9LGmgpFGS5qT8tZJ+kc4/W9I3UvqBKf+jki6X1LeNz3Z02j9H0rkl6SsknSfpEWDPSq9VtWz+nkZeXtTnne0lL9YzZFhjFWtkRePvUHV1NGrlvA72BXBAJ84zDtg5Ip5ulX4MMC0ifpZarv3XOknEckmzgI+QvSnjkyl/o6SJwEkR8ZSkPYCLS+q0FbBXRDRJuhE4OSLukTQAaP333onAKGBsRKyWNFhSP2AKcGBEPCnpN8DXyN6WBGTdM8C5wO7Aq8Btkj4VEdcDG5O9c++01hdC0onpnPRb++OaWY4V9YGg/bvwPA+0EcQhe3/d5emFFddHxKw28vyebFqAO8neb3dxCsh7AdeWvLmotMV8bUS0/B18D/BLSVcCf4qIha3ednQQcGl6BRMRsVTSrsDTEfFkynMF2Rzs/1Vy3AeBuyLiZYBU/n5k79trAv7Y1oWIiInARIBNNDgXX41XFtczdPjb72wPGdbIkhfrq1gjK5pe/x0Kcv2IfiU3O7vCyrYSI2I6WfB7AZgi6XhJh5d0hYwnezHphDT3y+7AHaneyyJibMny/rbOFxHnAF8GNgLuaemC6WZvlfyQ5N4Ts/ozYvTbbDlyFXX1zTQctoz7bvMIU6vcBvEdynEfeUVPdnYXSdsACyNiUuqDHhcRpwJ/bpVvBvAr4KYUIF+T9LSkz0bEtcqa2B9IL4ZufY73RsSjwKOSPgjsAJS2/P8KfFXSnS1dK8ATwChJ74uI+cBxwP+2KvoB4IL0lutXgaOBX6/3RamC5iZx0Y9HcPZVC6iphduuGcyzT3qCy1L/8bVtmH3vAJYvrePY3XfkuNMWM+GYpdWuVm5sCN+hQnat9JAG4HuSGoEVwPHt5Ps92TzoDSVpxwKXSDodqCebnfFdgRw4VdL+ZEP15wJ/IZtbvcVkYDtgdqrHpIi4UNIJZF03dWRdQJeWFhoRL6ZZIe8km1js5oi4odIPnjcz7tiEGXdsUu1q5NYPL3m22lXIvV7/HcpxIFeUmUA3tXaPBbaNiLMkbQ28JyIe6IkK9mabaHDsoQOrXY3cmraorVsmVurg4WOrXYXc+1tc92BErNezH31HjoytvvXtivIu+N5p632+zqqkj/xisiF0R6ft14GLuq1GZmY5U+nDQNXqfqmka2WPiBgn6WGAiHhVUp9yB5mZ9So5HrVSSSBvTGO8A0DSUKo2NYyZWXXk+WZnJV0rF5CNItlC0s/IprA9u1trZWaWN0UefhgRV0p6kGwqWwGfiojHur1mZmZ5UcX+70pU8mKJrYE3gBtL0yLiue6smJlZrhQ5kAM3s+YlzP2A0WQPzOzUjfUyM8sV5fjOYCVdK7uUbqeZD7/ebTUyM7NO6fSTnRHxUJpt0Mxsw1HkrhVJ3ynZrCGbknZRt9XIzCxvin6zExhYsr6arM+8zSlazcx6raIG8vQg0MCI+G4P1cfMLJ+KGMgl1aVpXffuyQqZmeWNyPeolY6e7GyZ3XCWpKmSjpP06ZalJypnZpYLXThplqQJkp6QND9Nhd1evs9IivSCnQ5V0kfeD3iF7H2YLePJA/hTBceamfUOXdC1krqrLwI+CiwEZkiaGhHzWuUbCHwLuL+ScjsK5FukEStzWBPAW+S4t8jMrBt0TdT7EDA/IhYASLoGOAyY1yrfv5O93P17lRTaUddKLTAgLQNL1lsWM7MNRie6VoZImlmynFhSzAjg+ZLthSltzXmyhy5HRsTNldatoxb5ixFxVqUFmZn1apW3yJes6xuCJNUAvwS+2JnjOgrk+Z1F3cysJ0WXjVp5ARhZsr1VSmsxENgZuCt7yybvAaZKOjQiZrZXaEeB3C+TNDNr0TV95DOAMZJGkwXwzwHHvHOKiOXAkJZtSXcB3+0oiEMHfeQRsXQ9K2xm1mt0xfDDiFgNnAJMAx4D/hARcyWdJenQda1bpyfNMjPbIHXRWL2IuAW4pVXaGe3kbaikTAdyM7Nyqvgat0o4kJuZlSGKP/uhmdkGz4HczKzoHMjNzArOgdzMrMB6wRuCzMzMgdzMrNjy/GIJB3LLrb1PPanaVci9YXfPr3YV8m+frinGXStmZkXmB4LMzHoBB3Izs+Lyk51mZr2AmvMbyR3IzczKcR+5mVnxuWvFzKzoHMjNzIrNLXIzs6JzIDczK7DwI/pmZoXmceRmZr1B5DeSO5CbmVXALXIzsyLzA0FmZsXnm51mZgXnQG5mVmSBb3aamRWdb3aamRWdA7mZWXH5gSAzs6KL8IslzMwKL79x3IHczKwS7loxMyuyANy1YmZWcPmN49RUuwJmZkWgqGwpW440QdITkuZL+kEb+78jaZ6k2ZJul7RNuTIdyM3MKqDmqGjpsAypFrgIOATYETha0o6tsj0MjI+IDwDXAf9Zrm4O5GZm5UQnlo59CJgfEQsi4m3gGuCwtU4VcWdEvJE27wO2Kleo+8jNzMrIHgiquJN8iKSZJdsTI2JiWh8BPF+ybyGwRwdlfQn4S7kTOpCbmVWi8tkPl0TE+PU9naTPA+OBj5TL60BuZlaBTrTIO/ICMLJke6uUtva5pIOAHwMfiYhV5Qp1IDcAxje8xkn/vojamuAvVw/mDxduWe0q9bg9dniOUz/9D2oU3HjfDvzu9t3W2n9Uw2z+5cOP0dRcw7IV/Tj76gb++epAxr3vBb55+L3v5Nt6i2X85DcHcvejo3v6I3Sr1fe/zVu/WgnNQf0n+9H38/3X2v/WBStoergRgHgriGXBwL9sTvPiJt780WtZ//FqqP9MP/p8aqMqfIL10HVvCJoBjJE0miyAfw44pjSDpN2Ay4AJEfFSJYX2eCCXtCIiBrSz7x8Rsdd6ln8WMD0i/taJYw4FdoyIczrIMxy4ICKOWJ/65VFNTXDy2S/ww89ty5IX6/n1LU9x37RBPPdUv2pXrcfUqJnTjriHUy/5BC8t25jJ3/kTf58zimf+udk7eZ5auDlfOu/TrGqs51N7z+XkQ+/jjCs+ykPzR/DFn2dfi4H93+IPP76GBx4ve3+qUKIpeOuXK+h//iA0tIY3vrKMur37UDt6TQjp9801/1u/fd2bND21GgBtXkP/SzdFfUS8Eaz8wqvU7dOHmiG1Pf451l3XzLUSEaslnQJMA2qByyNibopbMyNiKvBzYABwrSSA5yLi0I7KzUWLXFJdRKxe3yAOEBFntHOO2ohoaueYqcDUMuUuAnpdEAfYfrc3WPRMHxY/1xeAu27YlD0PXr5BBfL3b/MSC5dswqJXNgHg9offx767PLNWIH9o/oh31uc+syUH7/7Uu8rZf9cF3PfYSFY11nd/pXtQ82OrqRlRS83wLPjWHdiX1X9/e61AXqrx9lX0/desxa56leyIzvQ150sXvVgiIm4BbmmVdkbJ+kGdLbNqww8lNUi6W9JUYF5KW5H+HSZpuqRZkuZI2rfVsYMkPSupJm1vLOl5SfWSpkg6IqU/I+lcSQ8Bn5X0cUmPS3pQ0gWSbkr5vijpwrQ+Je37h6QFJWWNkjQnrddK+kWq22xJ30jpZ0iakdInKv2c5t3m72nk5UV93tle8mI9Q4Y1VrFGPW/ooDd46dU1LcqXlm3M0EEr283/Lx9+nPse2/pd6Qft9n/89aH3dUsdq6n55WZqtlgTLmqG1hBL2o7IzYubiEVN1I5b82PW/M8mVn7hVVZ8Zil9jt2oYK1xILJXvVWyVEO1x5GPA74VEdu1Sj8GmBYRY4FdgVmlOyNieUpruZv7yZS/rejzSkSMA64n63c6JCJ2B4Z2UK9hwD6p3La6W04ERgFj06D9K1P6hRHxwYjYGdgoHW+9zMd2f5IdRr7MVXfsulb65pusZNvhS7m/l3WrdFbj7auoa+iLate0Y2q2rGXjKzZj42s2o/HWVTQvLWCzPKKypQqqHcgfiIin20ifAZwg6Uxgl4h4vY08vweOSuufS9ttaUnfAVhQcr6rO6jX9RHRHBHzgLbu+h0EXBYRqwEiYmlK31/S/ZIeBQ4Admp9oKQTJc2UNLORsjeje8Qri+sZOvztd7aHDGtkyYu9q2ugnJeX92eLzVa8s73Fpit5efnG78o3fruFfOFjD/P9yRNobFq7VXnA2AVMnz2KpuaCtTYrUDO0huaX1gTf5peb0ZC2w8fq21dRf1DftssZUkvN6FqaHingX3xd80BQt6h2IG/zb9eImA7sR3ZXd4qk4yUdnrpaZkkaT9anPUHSYGB34I7OnKOM0ghbUfeIpH7AxcAREbELMAl4VydzREyMiPERMb6etr/sPe2JWf0ZMfptthy5irr6ZhoOW8Z9tw2qdrV61OPPbcFWQ5YzbPBr1NU2ceBu8/n7nLWnuBgzYgnfP/Ju/m3SBJatePeoi4+Om8/femG3CkDNDnU0L2yieVET0Risvn0Vdfv0eVe+pmdXE68HNTuv6TtvfqmJWJVFuHi9mabZjdRsXbwfOzU3V7RUQy5udraWJolZGBGTJPUFxkXEqcCfW+WbAfwKuKm9G5klngC2lTQqIp5hTWt+XfwV+KqkO9Nd6MGsuYWzRNIAshuj163HOXpMc5O46McjOPuqBdTUwm3XDObZJzecG50ATc01nP/HffjlSbdQWxPcdP/2PL14MF8+ZAaPPzeUv88dxcmH3sdGfRv56Ql/BeCfrw7g3yZPAOA9g19ni01X8PD/Da/mx+g2qhP9vj2AN05bDs1Q/4l+1I6uY9XkldTuUEfdPlmjZPXtq6g/sC+lt4ean21i1YWvZU2igD5H96f2vbkMPe0Lcn2TNq9XswH4nqRGYAVwfDv5fg9cm/J3KCLelPR14FZJK8m6b9bVZGA7YHaq46SIuFDSJGAOsHg9y+9xM+7YhBl3bFLtalTVvY9tzb2tbmBO/ssH31k/9ZL2b3ksXjqQT515XLfVLQ/q9uzDgD0Hr5XW98trdz/1/dd3d0fVfbAPdVe8u/VeJCK66oGgbtHjgbxlDHlE3AXc1c6+K4ArKijrOlp1fUTEF0vWR7U65M6I2CGNJrkImJnyTQGmtD6+VZ2eAXZO66uB76SlNO/pwOnl6m1mBZTjQF7tPvKe9hVJs4C5wCCyUSxmZuXleNRKXrtWukVEnA+cX+16mFnBuI/czKz4qjUipRIO5GZmZVWv26QSDuRmZuUEDuRmZoWX354VB3Izs0p4HLmZWdE5kJuZFVgENOW3b8WB3MysEm6Rm5kVnAO5mVmBBdAF7+zsLg7kZmZlBYT7yM3MiivwzU4zs8JzH7mZWcE5kJuZFZknzTIzK7YAPI2tmVnBuUVuZlZkfkTfzKzYAsLjyM3MCs5PdpqZFZz7yM3MCizCo1bMzArPLXIzsyILoqmp2pVolwO5mVk5nsbWzKwXyPHww5pqV8DMLO8CiOaoaClH0gRJT0iaL+kHbezvK+n3af/9kkaVK9OB3MysnEgvlqhk6YCkWuAi4BBgR+BoSTu2yvYl4NWIeB9wPnBuueo5kJuZVSCamipayvgQMD8iFkTE28A1wGGt8hwGXJHWrwMOlKSOClXkeEhNbyfpZeDZatejlSHAkmpXIsd8fcrL2zXaJiKGrk8Bkm4l+1yV6Ae8VbI9MSImpnKOACZExJfT9nHAHhFxSsm55qQ8C9P2/6U87V5T3+ysovX9cnUHSTMjYny165FXvj7l9cZrFBETql2Hjrhrxcys57wAjCzZ3iqltZlHUh0wCHilo0IdyM3Mes4MYIyk0ZL6AJ8DprbKMxX4Qlo/ArgjyvSBu2vFWptY7QrknK9Peb5G7YiI1ZJOAaYBtcDlETFX0lnAzIiYCvw38FtJ84GlZMG+Q77ZaWZWcO5aMTMrOAdyM7OCcyAvEEkrOtj3j24874+6q+zOqtY1qISk4ZKuW8dj75LUpUP2uvtaSTpL0kGdPObQth5Lb5Vnna/jhsp95AUiaUVEDGiVVhcRq3v6vNVSrWvQ3eeTdBfw3YiYWWH+2ojo8DHCKn5fytbNupZb5AUkqUHS3ZKmAvNS2or07zBJ0yXNkjRH0r5tHL+TpAdSntmSxqT0z5ekXyapVtI5wEYp7cqU7zup7DmSTk1pG0u6WdIjKf2olH6GpBkpbWK5R4174hpIGiTpWUk1JXV/XlK9pPdKulXSg6n8HVKeKZIulXQ/8J+SPpLKnyXpYUkDJY1KT+WRrt0v0vlnS/pGSj8w5X9U0uWS+rbx2Y5O++dIOrckfYWk8yQ9AuyZg2s1RdmTikh6RtK5kh4CPivp45IeT9fxAkk3pXxflHRhyTW9QNI/JC0oKauS69gt36vCiggvBVmAFenfBmAlMLqNfacBP07rtcDANsr5NXBsWu8DbAS8H7gRqE/pFwPHl5ad1ncHHgU2BgYAc4HdgM8Ak0ryDUr/Di5J+y3wLzm5BjcA+6f1o4DJaf12YExa34NsDC/AFOAmoDZt3wjsndYHkA3lHQXMSWlfI5sno67lOpA9uv08sF1K+w1walq/CxgPDAeeA4amMu8APpXyBHBkjq7VFOCItP4M8P203vI5R6ftq4Gb0voXgQtLjr+WrEG5I9kcJJS7jt3xvSr64hZ5cT0QEU+3kT4DOEHSmcAuEfF6G3nuBX4k6d/I5qF4EziQLEjPkDQrbXvqgH0AAAXaSURBVG/bxrH7AH+OiJURsQL4E7AvWXD/aGqV7RsRy1P+/ZVNxfkocACw0zp/4ndbn2vwe7KgBNk43d9LGgDsBVybrsFlwLCSY66NNV0G9wC/lPRNYNN4d3fFQcBlLekRsRTYHng6Ip5Mea4A9mt13AeBuyLi5XTslSV5moA/tnUhKtCl16qdc7Sk7wAsKDnf1R3U6/qIaI6IecCWbexv6zpC936vCseBvLhWtpUYEdPJ/sd/AZgi6XhJh5d0A4yPiKuAQ4E3gVskHQAIuCIixqZl+4g4s9LKpOA0jiyg/zT96duPrGV/RETsAkwia611lXW+BmRPz02QNJjsB+wOsv8flpVcg7ER8f62zhcR5wBfJvtr5p6WLphu9lase99zV1+ris9RxqqS9Yq6R3rge1U4DuS9jKRtgH9GxCRgMjAuIv5cEphmStqWrMV0AdmfzR8g61I4QtIWqZzBqSyARkn1af1u4FOS+kvaGDgcuFvScOCNiPgd8HOyoN7yP9eS1No9otsvAJVdg/TXxAzgV2R/9jdFxGvA05I+m8qRpF3bOcd7I+LRiDg3ldM6kP8V+KqyuTJIQfAJYJSk96U8xwH/2+q4B4CPSBqibO7qo9vI02XW9VqVKfYJYFuteSHCUe1nLaut61iV71We+RH93qcB+J6kRmAFcHwbeY4Ejkt5FgNnR8RSSacDt6UbW43AyWTT7E4EZkt6KCKOlTSFLOBA1l/6sKSDgZ9Lak7Hfi0ilkmaBMxJ55nRTZ+5tQbKXwPIugKuTflbHAtckq5FPdl80Y+0ceypkvYHmsnuE/yFtbthJgPbkV23RrL7BxdKOoGs66aO7HpcWlpoRLyobHjenWQt1Jsj4oZKP/g6aGDdr1WbIuJNSV8HbpW0kvX7797edazG9yq3PPzQzLqcpAERsSKNJrkIeCoizq92vXord62YWXf4SrphPJdsGtbLqlyfXs0tcjOzgnOL3Mys4BzIzcwKzoHczKzgHMgt1yQ1ac08INdK6r8eZZXODTJZ0o4d5G2QtNc6nOMZSe9623p76a3ytDtbYTv5z5T03c7W0XofB3LLuzfTgyk7A28DJ5XubHlQpLMi4svpsfD2NJA9rm+Wew7kViR3A+9Tq9n80gx5P0+z4c2W9FV458nMCyU9IelvwBYtBalk/m9JEyQ9pGzmxtvTE4knAd9Ofw3sK2mopD+mc8yQtHc6dnNJt0maK2kyFTxmLul6ZbMCzpV0Yqt956f02yUNTWltzsho1sJPdlohpJb3IcCtKWkcsHNEPJ2C4fKI+KCyaWHvkXQb2ayM25PNrLcl2RSul7cqdyjZXB37pbIGp6dcLyWbIfAXKd9VwPkR8XdJW5O9PPf9wE+Av0fEWZI+AXypgo/zr+kcG5FNUvbHiHiFbEbJmRHxbUlnpLJPIXuy9qSIeErSHmTzjBywDpfReikHcsu7jdKDJZC1yP+brMujdDa/jwEfaOn/JnsAZQzZZFBXp7lBFklqa7KnDwPTW8oqmV2vtYOAHbVm2utN0jwf+wGfTsfeLOnVCj7TNyUdntZHprq+Qva4f8sMgr8D/qS1Z2RsOf5dc5jbhs2B3PLuzYgYW5qQAlrpTHsCvhER01rl+3gX1qMG+HBEvNVGXSomqYHsR2HPiHhD2ZuB2pu5LyiZkbGzFbYNh/vIrTeYBnxNaYZGSdspm5lxOnBU6kMfBuzfxrH3AftJGp2OHZzSXwcGluS7DfhGy4aklsA6HTgmpR0CbFamroOAV1MQ34HsL4IWNayZye8Ysi6bimdktA2XA7n1BpPJ+r8fUvaKsMvI/tr8M/BU2vcbshdqrCUiXgZOJOvGeIQ1XRs3Ai3zcu8LfBMYn26mzmPN6Jn/R/ZDMJesi+W5MnW9FaiT9BhwDtkPSYuVwIfSZzgAOCulHwt8KdVvLnBYBdfENiCea8XMrODcIjczKzgHcjOzgnMgNzMrOAdyM7OCcyA3Mys4B3Izs4JzIDczK7j/DyzHjeo/PuTYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, LR_testing_predictions, normalize='true')\n",
    "class_names = np.unique(y)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix, class_names)\n",
    "\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left is the `true` label, and on the bottom is the `predicted` label. This confusion matrix is normalized, so we get percentages. Basically, we want the diagonal from the top left to the bottom right to be as close to 1 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# Here's a cell just to play around with. Feel free to set your own flower features and predict on it.\n",
    "\n",
    "dummy_X = [[6.3, 2.9, 5.6, 1.8]]\n",
    "dummy_prediction = LR_classifier.predict(dummy_X)\n",
    "print(dummy_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Support Vector Machine](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
    "\n",
    "A Support Vector Machine uses the `kernel trick` to try and project the data points in such a way that they are linearly separable. In layman's terms, it tries to transform the dimensions of the data points so that it can fit a hyperplane to separate them as best as possible. Imagine being able to draw a line that perfectly separates your `0`'s and `1`'s. What's a hyperplane? A [hyperplane](https://en.wikipedia.org/wiki/Hyperplane) is a subspace that's `n-1` dimensions. For example, if we're working in a 2d space, the hyperplane is a 1d subspace, hence a line. If we're working in a 3d space, the hyperplane is a 2d subspace, so a rectangle of some sort. For an SVM, the most import data points are the `support vectors`. Those are the data points closest to the hyperplane. As you can imagine, points that are very far away from the hyperplane shouldn't really have much effect on how the hyperplane adjusts. It's the close data points that really provide a lot of information.\n",
    "\n",
    "[Doc page](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "*Note that scikit-learn has multiple implementations of an SVM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_classifier = SVC(decision_function_shape='ovo')\n",
    "SVM_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll predict here\n",
    "## Predict on training data\n",
    "\n",
    "SVM_training_predictions = SVM_classifier.predict(X_train)\n",
    "accuracy_score(y_train, SVM_training_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_testing_predictions = SVM_classifier.predict(X_test)\n",
    "accuracy_score(y_test, SVM_testing_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that just because one algorithm has a lower/higher accuracy than another on one dataset, doesn't make it better/worse. The whole \"machine learning\" process is a lot of trial and error, which means plugging the dataset into many different algorithms. Different algorithms work better with different datasets. It's up to you to explore them and find the best one.\n",
    "\n",
    "That being said, some algorithms are more \"explainable\" than others. It's much easier to explain the weights in a Logistic Regression, than it is to explain those of a huge Neural Network.\n",
    "\n",
    "Another thing to keep in mind. If a simple algorithm achieves the same results as a complex algorithm, choose the simple one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "\n",
    "A Decision Tree algorithm has many ways to be created. Essentially, it all comes down to which feature we choose to split on, and how we choose that feature. The usual way is to use the `gini index`, which is a way to measure how the probability of an object being classified to a particular class. We calculate the gini index for each column. We prefer to use the column with the least gini index as the root node of our tree. This process of choosing a feature to split on continues until we run out of features, or we hit a set limit. The other popular way of splitting is to calculate the `entropy` of each feature. Both of these ways eventually lead to our `Information Gain` metric.\n",
    "\n",
    "[Doc page](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT_classifier = DecisionTreeClassifier(criterion='gini', )\n",
    "DT_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll predict here\n",
    "## Predict on training data\n",
    "\n",
    "SVM_training_predictions = SVM_classifier.predict(X_train)\n",
    "accuracy_score(y_train, SVM_training_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_testing_predictions = SVM_classifier.predict(X_test)\n",
    "accuracy_score(y_test, SVM_testing_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Random Forest](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "A Random Forest is essentially a bunch of trees. We split up the data, and train separate trees on each set. During prediction time, we take the majority vote of the trees as the predicted label. Single decision trees have a tendency to \"overfit\" on a training set. That is, a Decision Tree does not do well when generalizing to new data. That's a whole different topic you can read more about [here](https://en.wikipedia.org/wiki/Overfitting). Essentially what happens is that the single tree gets too deep, and too specific, to perfect its predictions on the training set. It then won't be well applicable to new data. It's been shown that Bagging techniques such as a Random Forest, can combat this.\n",
    "\n",
    "[Doc page](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use 5 trees\n",
    "RF_classifier = RandomForestClassifier(n_estimators=5)\n",
    "RF_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916666666666667"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll predict here\n",
    "## Predict on training data\n",
    "\n",
    "RF_training_predictions = RF_classifier.predict(X_train)\n",
    "accuracy_score(y_train, RF_training_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_testing_predictions = RF_classifier.predict(X_test)\n",
    "accuracy_score(y_test, RF_testing_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "## So that was an overview of some simple classification algorithms from scikit-learn. There's plenty more where that came from. Now let's look at something more \"modern\".\n",
    "\n",
    "Let's use the [Keras](https://keras.io/) library to create a Neural Network. It's most definitely overkill for the IRIS dataset, but since you've already made it this far, might as well finish it.\n",
    "\n",
    "First, we'll have to do some transformations on the data, as `Keras` works exclusively with numbers only. It's not meant to support the string `Iris-setosa` as a label.\n",
    "\n",
    "Labels will have to be [one-hot encoded](https://en.wikipedia.org/wiki/One-hot), and we'll scale our features to mean 0 and variance 1, as Neural Networks usually like that better. The scaling of the features also helps for larger datasets which contain larger numbers. Scaling down helps not to blow up the memory on a computer.\n",
    "\n",
    "### Small note on one-hot encoding\n",
    "One hot encoding creates a fair representation of our labels that Keras can understand. Since Keras only understands numbers, you may think, \"why not just make `Iris-virginica, Iris-versicolor, Iris-setosa` to `1,2,3` respectively?\". That's because when predicting labels, and evaluating the cost function (your prediction vs the actual, hence the `error`), there's a distance metric involved. As humans, we understand that each of those species are equally different from each other. But if you were to feed in `1,2,3` to an algorithm, you would be telling it that the second index (2) is closer to the third index (3) than the first index (1) is. That is, you'd be saying that `Iris-versicolor` is closer to `Iris-setosa` than `Iris-virginica` is, which is not correct. The best way to encode each species (label) is with one-hot encoding. We'd turn the species into `[0,0,1],[0,1,0],[1,0,0]`. That way, they are all equidistant from each other, hence it's a fair representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "[[ 0.43  0.8   0.93  1.45]\n",
      " [-1.14 -1.51 -0.26 -0.26]\n",
      " [-1.63 -1.74 -1.4  -1.18]\n",
      " [ 0.55  0.57  0.54  0.53]\n",
      " [ 1.4   0.34  0.54  0.26]\n",
      " [ 1.04  0.11  0.36  0.26]\n",
      " [ 2.25 -0.59  1.67  1.05]\n",
      " [-0.05 -0.82  0.76  0.92]\n",
      " [-1.14 -0.12 -1.34 -1.31]\n",
      " [ 1.28  0.11  0.93  1.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# One hot encode the labels\n",
    "enc = OneHotEncoder()\n",
    "y_encoded = enc.fit_transform(y[:, np.newaxis]).toarray()\n",
    "print(y_encoded[:10])\n",
    "\n",
    "print()\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.20, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "n_features = X_scaled.shape[1]\n",
    "n_classes = y_encoded.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=n_features, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120/120 [==============================] - 0s 657us/step - loss: 0.9149 - accuracy: 0.7333\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.8482 - accuracy: 0.8000\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.7840 - accuracy: 0.8333\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.7230 - accuracy: 0.8417\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.6666 - accuracy: 0.8417\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.6160 - accuracy: 0.8417\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.5720 - accuracy: 0.8333\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.5340 - accuracy: 0.8333\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5016 - accuracy: 0.8250\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4748 - accuracy: 0.8417\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4513 - accuracy: 0.8333\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4317 - accuracy: 0.8333\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4142 - accuracy: 0.8417\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.4001 - accuracy: 0.8500\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3864 - accuracy: 0.8583\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3744 - accuracy: 0.8667\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3633 - accuracy: 0.8833\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3536 - accuracy: 0.8917\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3442 - accuracy: 0.8917\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3359 - accuracy: 0.9083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1400cff28>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=5, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Careful here. You might be enticed by the accuracy going up along with the number of epochs, which would make you think that if you keep increasing the number of epochs, that you'll get a really godd accuracy. Keep in mind that this is `training` accuracy. And as said before on the topic of `overfitting`, which Neural Networks are notorious for, if you train too much on your training set, your network won't be able to generalize to new datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2867014706134796\n",
      "Test accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of this tutorial. Stay tuned for more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
